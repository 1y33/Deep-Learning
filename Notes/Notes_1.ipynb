{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEf+u0adUYLGSmrMa+8mfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1y33/Deep-Learning/blob/main/Notes/Notes_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Loss function and Calculatin the prediction probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "TTeT9eu9rESr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Classification :"
      ],
      "metadata": {
        "id": "6KTevZuKrw87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Setting up a loss function\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss() # It has an sigmoid layer | BCELoss() doesn't have one\n",
        "\n",
        "# How to create probabilities for a Binary Classification\n",
        "pred_probabilities = torch.round(torch.sigmoid(model(x)))\n",
        "\n",
        "# To calculate the loss we need to use the raw logits from the model\n",
        "y_logits = model(X)\n",
        "loss=loss_fn(y_logits,y)"
      ],
      "metadata": {
        "id": "jr7R-MAsryAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-class Classification"
      ],
      "metadata": {
        "id": "cTcegPXdr-K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Setting up a loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# How to create probabilities for a Multiclass Classification\n",
        "pred_probabilities = torch.softmax(model(x),dim=1).argmax(dim=1)\n",
        "\n",
        "# To calculate the loss we need to use the raw logits from the model\n",
        "y_logits = model(X)\n",
        "loss=loss_fn(y_logits,y)\n"
      ],
      "metadata": {
        "id": "nmKbDmXgtba_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Downdloading and loading data sets\n",
        "\n"
      ],
      "metadata": {
        "id": "gK_xyg2uvBxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What to import"
      ],
      "metadata": {
        "id": "uQ8IIvI9vOzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "oiwCUJslvM78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to work with data\n",
        "\n",
        "In the example we will use FashionMNIST as our data set . Depending on the dataset we use we need different atributes"
      ],
      "metadata": {
        "id": "068NPNVQvRKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to?\n",
        "    train=True, # get training data\n",
        "    download=True, # download data if it doesn't exist on disk\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
        "    target_transform=None # you can transform labels as well\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#\n",
        "#               Preparing the dataloader\n",
        "#       Here we will create the batches for our dataset\n",
        "\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
        "    batch_size=BATCH_SIZE, # how many samples per batch?\n",
        "    shuffle=True # shuffle data every epoch?\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False # don't necessarily have to shuffle the testing data\n",
        ")"
      ],
      "metadata": {
        "id": "jH8aMm9HvJ90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Saving and Loading our model"
      ],
      "metadata": {
        "id": "F_zx6X7svqPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model's state_dict()"
      ],
      "metadata": {
        "id": "iU1akUeGwit7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Creating the directorty\n",
        "model_path = Path(\"model\")\n",
        "model_path.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "# Creating the path\n",
        "model_name = \"random_name\"\n",
        "model_save_path=model_path/model_name\n",
        "\n",
        "# Saving the model\n",
        "torch.save(obj=model.state_dict9(),\n",
        "           f=model_save_path)"
      ],
      "metadata": {
        "id": "GzU6r-BwwEBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model's state_dict()\n",
        "\n",
        "Important : We need to create another model as the model we saved it"
      ],
      "metadata": {
        "id": "DkWz8nkZwD_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = cnn() # the same model class as the model we save ( example : cnn () # random model)\n",
        "new_model=torch.load_state_dict(torch.load(f=model_save_path))"
      ],
      "metadata": {
        "id": "v-eAreKMwD6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. CNN explained\n",
        "\n",
        "Some functions from a cnn architecture , resource from : https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "The format will be a cheat sheet style"
      ],
      "metadata": {
        "id": "Aef-E56MxXd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layers\n",
        "\n",
        "nn.Conv2d(\n",
        "    in_channels = x # what eneters the layer\n",
        "    out_channels = y # what exits the layer\n",
        "    kernel_size = 3 # mostly 3 , is used for the shape of the slider over the image\n",
        "    padding = 1 # kernel expends over the activation area\n",
        "    stride = 1 # the kernel will jump over 1 activation dot\n",
        "    )\n",
        "\n",
        "nn.MaxPool2d(\n",
        "    kernel_size=2 # will chose from a 2x2 are the biggest number ( from an output of 60X60 will create 30X30 )\n",
        ")\n",
        "\n",
        "nn.Flatten () # will flatten all the dimensions into a single one ( from 5x5x2 will be only 50 )\n",
        "\n",
        "nn.Softmax() # creates the prediction probabilities"
      ],
      "metadata": {
        "id": "xoaV2NKExZr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of construction blocks"
      ],
      "metadata": {
        "id": "uei649XzzPmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.block_1 = nn.Sequential(\n",
        "    nn.Conv2d(....)\n",
        "    nn.ReLU(inplace=True) # We use it because we dont save the past data\n",
        "    nn.Conv2d(....)\n",
        "    nn.ReLU()\n",
        "    nn.MaxPool2d(.)\n",
        ")\n",
        "\n",
        "self.block_2 = nn.Sequential(\n",
        "    nn.Conv2d(....)\n",
        "    nn.ReLU(inplace=True)\n",
        "    nn.Conv2d(....)\n",
        "    nn.ReLU()\n",
        "    nn.MaxPool2d(.)\n",
        ")\n",
        "\n",
        "self.Classification = nn.Sequential(\n",
        "    nn.Flatten()\n",
        "    nn.Linear(..)\n",
        ")"
      ],
      "metadata": {
        "id": "6WMlenL3zSsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BEoK9GOgzScE"
      }
    }
  ]
}