{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:25:35.393089100Z",
     "start_time": "2024-03-23T20:25:33.771448500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## HYPER PARAMS\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"ctx_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" :0.1,\n",
    "    \"qKv_bias\" : False,\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"ctx_len\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_block = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_block(x)\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self,normalized,eps= 1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:25:35.407818800Z",
     "start_time": "2024-03-23T20:25:35.398092300Z"
    }
   },
   "id": "322336887c88d47b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   34,  1045,   304,   285,   533,  6478],\n",
      "        [ 5840, 21043,   270,   285,  1872,  2341]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([2, 6, 50257])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken \n",
    "import torch\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Cata e mare boss\"\n",
    "txt2 = \"Am mintit mai sus\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch,dim=0)\n",
    "print(batch)\n",
    "\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "logits.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:25:36.085993100Z",
     "start_time": "2024-03-23T20:25:35.407818800Z"
    }
   },
   "id": "15a9216d8140549f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 0.6126, 0.9560, 0.0000, 0.0000, 0.0868],\n        [0.0000, 0.3552, 0.6973, 0.0000, 0.0000, 0.3049]],\n       grad_fn=<ReluBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(36)\n",
    "batch_example = torch.randn(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:25:36.102537200Z",
     "start_time": "2024-03-23T20:25:36.084638100Z"
    }
   },
   "id": "48aa0bf28b6a713b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.2759],\n         [0.2262]], grad_fn=<MeanBackward1>),\n tensor([[0.1680],\n         [0.0797]], grad_fn=<VarBackward0>))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "var = out.var(dim=-1,keepdim=True)\n",
    "\n",
    "mean,var"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:25:36.135241400Z",
     "start_time": "2024-03-23T20:25:36.100545600Z"
    }
   },
   "id": "6666d3e780839175",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[4.9671e-08],\n         [0.0000e+00]], grad_fn=<MeanBackward1>),\n tensor([[1.0000e+00],\n         [1.0000e+00]], grad_fn=<VarBackward0>))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=True)\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1,keepdim=True)\n",
    "var = out_norm.var(dim=-1,keepdim=True)\n",
    "mean,var"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:25:36.136241800Z",
     "start_time": "2024-03-23T20:25:36.112960Z"
    }
   },
   "id": "63688686791b1d28",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        var = x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:25:36.137239700Z",
     "start_time": "2024-03-23T20:25:36.124293Z"
    }
   },
   "id": "308c9c91d6270f52",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))* (x+0.044715 * torch.pow(x,3))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:25:36.173386200Z",
     "start_time": "2024-03-23T20:25:36.134239400Z"
    }
   },
   "id": "c7d2c19853757cb7",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
    "            GeLU(),\n",
    "            nn.Linear(cfg[\"emb_dim\"]*4,cfg[\"emb_dim\"]),\n",
    "            nn.Dropout(cfg[\"drop_rate\"])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:27:58.850807Z",
     "start_time": "2024-03-23T20:27:58.841293100Z"
    }
   },
   "id": "4f634e7af10930ba",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,\n",
    "                 block_size,dropout,num_heads,qKv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.block_size = block_size\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        self.w_keys = nn.Linear(self.d_in,self.d_out,bias=qKv_bias)\n",
    "        self.w_query = nn.Linear(self.d_in,self.d_out,bias = qKv_bias)\n",
    "        self.w_values = nn.Linear(self.d_in,self.d_out,bias = qKv_bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_prj = nn.Linear(d_out,d_out)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(block_size,block_size),diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        b,num_tokens, d_in = x.shape\n",
    "        keys = self.w_keys(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_values(x)\n",
    "        \n",
    "        keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        \n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        mask_unsqueeze = mask_bool.unsqueeze(0).unsqueeze(0)\n",
    "        attn_scores.masked_fill_(mask_unsqueeze,-torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5,dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vec = self.out_prj(context_vec)\n",
    "        \n",
    "        return context_vec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:44:24.546913100Z",
     "start_time": "2024-03-23T20:44:24.535331900Z"
    }
   },
   "id": "6a75ae9b00b69740",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            block_size=cfg[\"ctx_len\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qKv_bias=cfg[\"qKv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:47:46.061558400Z",
     "start_time": "2024-03-23T20:47:46.055809Z"
    }
   },
   "id": "4991ac73c00c7113",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 4, 768]), torch.Size([2, 4, 768]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,4,768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = block(x)\n",
    "out.shape,x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:48:16.450478100Z",
     "start_time": "2024-03-23T20:48:16.395049500Z"
    }
   },
   "id": "45c7a1e37f645c42",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"ctx_len\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_block = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        #x = self.drop_emb(x)\n",
    "        x = self.trf_block(x)\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:50:54.915528400Z",
     "start_time": "2024-03-23T20:50:54.905473500Z"
    }
   },
   "id": "67c3ca4f3647cd3b",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 6, 50257])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:51:26.452077500Z",
     "start_time": "2024-03-23T20:51:25.392407600Z"
    }
   },
   "id": "67588b8dd838c96b",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "163009536"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:51:59.312578600Z",
     "start_time": "2024-03-23T20:51:59.305564100Z"
    }
   },
   "id": "21734636695cd887",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "124412160"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_params =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "trainable_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:53:57.757654500Z",
     "start_time": "2024-03-23T20:53:57.745616100Z"
    }
   },
   "id": "64ade6a2c6324faf",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_text_simple(model,idx,max_new_tokens,context_scale):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_scale:]\n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:,-1,:]\n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "        idx_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "        idx = torch.cat((idx,idx_next),dim=1)\n",
    "    return idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T20:59:37.649085700Z",
     "start_time": "2024-03-23T20:59:37.638542400Z"
    }
   },
   "id": "8bd8c616151228a",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Drake  fellowship Laos makers disproonssmoking systemd supraENTS cruiser'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(False)\n",
    "\n",
    "start_context = (\"Drake \")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_context = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_context,\n",
    "    max_new_tokens=10,\n",
    "    context_scale=GPT_CONFIG_124M[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "decoded_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T21:13:55.615101400Z",
     "start_time": "2024-03-23T21:13:54.799604800Z"
    }
   },
   "id": "437407530108990b",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3eed40c06c361422"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
